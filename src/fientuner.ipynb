{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhatt/miniconda3/envs/thesis/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from models import FineTuner\n",
    "from generate_metrics import Metric\n",
    "from paths import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "## Setting current work directory for relative paths\n",
    "os.chdir(CURRENT_WD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_ckpt = {'imdb':{\n",
    "                        'bert-base-uncased' : '/home/bhatt/ishan/TUM_Thesis/data/models/bert-base-uncased-finetuned-imdb_2023-12-19/checkpoint-1500',\n",
    "                        'bert-base-cased' : '/home/bhatt/ishan/TUM_Thesis/data/models/bert-base-cased-finetuned-imdb_2023-12-19/checkpoint-1500',\n",
    "                        'distilbert-base-uncased' : '/home/bhatt/ishan/TUM_Thesis/data/models/distilbert-base-uncased-finetuned-imdb_2023-12-20/checkpoint-1500',\n",
    "                        'roberta-base' : '/home/bhatt/ishan/TUM_Thesis/data/models/roberta-base-finetuned-imdb_2023-12-20/checkpoint-1500',\n",
    "                        'albert-base-v2' : '/home/bhatt/ishan/TUM_Thesis/data/models/albert-base-v2-finetuned-imdb_2024-01-09/checkpoint-1500'\n",
    "                            },\n",
    "                    '4chan' : {\n",
    "                        'bert-base-uncased' : '/home/bhatt/ishan/TUM_Thesis/data/models/bert-base-uncased-finetuned-4chan_2023-12-29/checkpoint-1500',\n",
    "                        'bert-base-cased' : '/home/bhatt/ishan/TUM_Thesis/data/models/bert-base-cased-finetuned-4chan_2023-12-29/checkpoint-1500',\n",
    "                        'distilbert-base-uncased' : '/home/bhatt/ishan/TUM_Thesis/data/models/distilbert-base-uncased-finetuned-4chan_2023-12-29/checkpoint-1500',\n",
    "                        'roberta-base' : '/home/bhatt/ishan/TUM_Thesis/data/models/roberta-base-finetuned-4chan_2023-12-29/checkpoint-1500',\n",
    "                        'albert-base-v2' : '/home/bhatt/ishan/TUM_Thesis/data/models/albert-base-v2-finetuned-4chan_2024-01-05/checkpoint-1500'\n",
    "                    },\n",
    "                    'random' : {\n",
    "                        'bert-base-uncased' : '/home/bhatt/ishan/TUM_Thesis/data/models/bert-base-uncased_random_init_2024-01-29',\n",
    "                        'bert-base-cased' : '/home/bhatt/ishan/TUM_Thesis/data/models/bert-base-cased_random_init_2024-01-29',\n",
    "                        'distilbert-base-uncased' : '/home/bhatt/ishan/TUM_Thesis/data/models/distilbert-base-uncased_random_init_2024-01-29',\n",
    "                        'roberta-base' : '/home/bhatt/ishan/TUM_Thesis/data/models/roberta-base_random_init_2024-01-29',\n",
    "                        'albert-base-v2' : '/home/bhatt/ishan/TUM_Thesis/data/models/albert-base-v2_random_init_2024-01-29'\n",
    "                    }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   3%|▎         | 2000/60558 [00:00<00:06, 9032.20 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 60558/60558 [00:02<00:00, 25260.91 examples/s]\n",
      "Map: 100%|██████████| 60558/60558 [00:07<00:00, 7872.79 examples/s]\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3140' max='3140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3140/3140 10:05, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.774000</td>\n",
       "      <td>2.533240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.572500</td>\n",
       "      <td>2.413598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.498400</td>\n",
       "      <td>2.378931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.441900</td>\n",
       "      <td>2.330365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.408900</td>\n",
       "      <td>2.300413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.368200</td>\n",
       "      <td>2.318638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.352400</td>\n",
       "      <td>2.322101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.305100</td>\n",
       "      <td>2.258545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.307400</td>\n",
       "      <td>2.251570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.274100</td>\n",
       "      <td>2.245518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.264900</td>\n",
       "      <td>2.259915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.256600</td>\n",
       "      <td>2.274113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.254300</td>\n",
       "      <td>2.219406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.238500</td>\n",
       "      <td>2.196738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.223000</td>\n",
       "      <td>2.243749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.212600</td>\n",
       "      <td>2.218581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.205200</td>\n",
       "      <td>2.234454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.211000</td>\n",
       "      <td>2.210381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.207500</td>\n",
       "      <td>2.182701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.202800</td>\n",
       "      <td>2.250748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/60558 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 60558/60558 [00:02<00:00, 21118.73 examples/s]\n",
      "Map: 100%|██████████| 60558/60558 [00:09<00:00, 6710.16 examples/s]\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3140' max='3140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3140/3140 10:13, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.685500</td>\n",
       "      <td>2.423603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.496600</td>\n",
       "      <td>2.385120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.412400</td>\n",
       "      <td>2.312890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.378800</td>\n",
       "      <td>2.267908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.334700</td>\n",
       "      <td>2.264436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.310000</td>\n",
       "      <td>2.248687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.279600</td>\n",
       "      <td>2.195790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.242800</td>\n",
       "      <td>2.211576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.241600</td>\n",
       "      <td>2.198925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.216100</td>\n",
       "      <td>2.181444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.212300</td>\n",
       "      <td>2.200105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.186100</td>\n",
       "      <td>2.157391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.190100</td>\n",
       "      <td>2.133482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.174100</td>\n",
       "      <td>2.144917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.159800</td>\n",
       "      <td>2.158152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.157800</td>\n",
       "      <td>2.133528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.155200</td>\n",
       "      <td>2.144392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.148700</td>\n",
       "      <td>2.135382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.138500</td>\n",
       "      <td>2.126035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.130500</td>\n",
       "      <td>2.175102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model\n",
      "Model Initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/60558 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 60558/60558 [00:02<00:00, 27943.95 examples/s]\n",
      "Map: 100%|██████████| 60558/60558 [00:06<00:00, 8980.73 examples/s]\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3140' max='3140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3140/3140 06:22, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.012300</td>\n",
       "      <td>2.757947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.796400</td>\n",
       "      <td>2.631540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.724500</td>\n",
       "      <td>2.623481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.672900</td>\n",
       "      <td>2.580163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.631600</td>\n",
       "      <td>2.529289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.589800</td>\n",
       "      <td>2.541869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.577000</td>\n",
       "      <td>2.531072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.522700</td>\n",
       "      <td>2.477293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.522500</td>\n",
       "      <td>2.469830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.490200</td>\n",
       "      <td>2.462274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.477700</td>\n",
       "      <td>2.462982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.473300</td>\n",
       "      <td>2.476037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.466300</td>\n",
       "      <td>2.426533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.449600</td>\n",
       "      <td>2.395319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.435800</td>\n",
       "      <td>2.448828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.426500</td>\n",
       "      <td>2.422215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.417800</td>\n",
       "      <td>2.447914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.426700</td>\n",
       "      <td>2.420443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.422400</td>\n",
       "      <td>2.402693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.418500</td>\n",
       "      <td>2.445223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model\n",
      "Model Initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/60558 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 60558/60558 [00:02<00:00, 28534.74 examples/s]\n",
      "Map: 100%|██████████| 60558/60558 [00:06<00:00, 9057.78 examples/s]\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3140' max='3140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3140/3140 11:48, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.369600</td>\n",
       "      <td>2.090790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.155700</td>\n",
       "      <td>2.047202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.104900</td>\n",
       "      <td>2.023410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.069500</td>\n",
       "      <td>1.949717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.033000</td>\n",
       "      <td>1.971839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.020400</td>\n",
       "      <td>1.917559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.999600</td>\n",
       "      <td>1.972926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.983000</td>\n",
       "      <td>1.926671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.958400</td>\n",
       "      <td>1.956942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.936400</td>\n",
       "      <td>1.930740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.935900</td>\n",
       "      <td>1.911259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.932300</td>\n",
       "      <td>1.947014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.928300</td>\n",
       "      <td>1.901900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.910300</td>\n",
       "      <td>1.885840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.898400</td>\n",
       "      <td>1.914946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.886300</td>\n",
       "      <td>1.889590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.890300</td>\n",
       "      <td>1.859552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.878800</td>\n",
       "      <td>1.906674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.880700</td>\n",
       "      <td>1.920798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.887900</td>\n",
       "      <td>1.928181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.weight', 'albert.pooler.bias']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/60558 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 60558/60558 [00:02<00:00, 23389.20 examples/s]\n",
      "Map: 100%|██████████| 60558/60558 [00:09<00:00, 6464.65 examples/s]\n",
      "You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3140' max='3140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3140/3140 12:04, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.102600</td>\n",
       "      <td>2.722781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.660700</td>\n",
       "      <td>2.648469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.599700</td>\n",
       "      <td>2.584096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.572100</td>\n",
       "      <td>2.561742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.509500</td>\n",
       "      <td>2.528055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.472900</td>\n",
       "      <td>2.528646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.473900</td>\n",
       "      <td>2.486352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.436300</td>\n",
       "      <td>2.496748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.411700</td>\n",
       "      <td>2.504485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.399000</td>\n",
       "      <td>2.484804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.388500</td>\n",
       "      <td>2.510876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.364900</td>\n",
       "      <td>2.457728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.362700</td>\n",
       "      <td>2.464216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.350200</td>\n",
       "      <td>2.450540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.347600</td>\n",
       "      <td>2.396514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.331600</td>\n",
       "      <td>2.406788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.325200</td>\n",
       "      <td>2.431005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.327000</td>\n",
       "      <td>2.450434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.306300</td>\n",
       "      <td>2.409128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.306700</td>\n",
       "      <td>2.430326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model_checkpoint in ['bert-base-uncased','bert-base-cased','distilbert-base-uncased','roberta-base','albert-base-v2']:\n",
    "# for model_checkpoint in ['roberta-base','albert-base-v2']:\n",
    "\n",
    "    # print('**********************************************************************')\n",
    "    # print('**********************************************************************')\n",
    "    # print('**********************************************************************')\n",
    "    # print('Current Model : ',model_checkpoint)\n",
    "    # ft = FineTuner(model_name=model_checkpoint, random_init=True)\n",
    "    # ft.save_model() ## Saving the random model\n",
    "    # del ft\n",
    "    ft = FineTuner(model_name=model_checkpoint,from_local=False, model_save_dir='None',local_model_path=pre_trained_ckpt['random'][model_checkpoint],ft_ds = None, random_init=False)\n",
    "    ft.finetune_model(dataset_name='4chan',train_size=10000)\n",
    "    del ft\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
