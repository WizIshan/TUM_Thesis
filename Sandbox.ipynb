{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67959507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5179e970-7c4a-4f5a-a2e0-809d86cf05ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4eab5f-e146-46b7-9ee1-8ef6720e7b4a",
   "metadata": {},
   "source": [
    "## DistilBERT for Masked Language Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0683bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = 'distilbert-base-uncased'\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1236772b-736f-423a-baff-c2dc22fe5549",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForMaskedLM.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe38c31-4d99-42c2-ac22-8fb3ef0be0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A thesis should be [MASK].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcb5667b-54ad-4355-bf84-c7ecd4af011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d69cde1d-d7ca-4de6-a690-481433be19cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1037, 9459, 2323, 2022,  103, 1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b0ea36b-2deb-4b0e-ab3c-0b620ae13e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b8ab0d5-f93e-488e-8e32-6f407209afe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "108d4f06-3069-464a-b1e3-dc047e6c5ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb86481e-de2f-46eb-a831-8cf5d49d26d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0]), tensor([5]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c55b96c-326c-457b-b0ae-f0fffd4c5199",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54e6d9f0-e4ed-4a89-85c7-4bcdde08664e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 30522])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe76ab11-37dc-4749-99ae-09f3c31e645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_logits = logits[0, mask_token_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc0cd02a-6e48-4722-92f7-a0e7e199ab01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A thesis should be considered.\n",
      "A thesis should be defended.\n",
      "A thesis should be written.\n"
     ]
    }
   ],
   "source": [
    "top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_3_tokens:\n",
    "    print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e3394f-8821-4d5e-b73a-511779314963",
   "metadata": {},
   "source": [
    "## DistilBERT for Contextual Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3ca5bc1-f4ba-4446-b3b7-c4519ca26187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_similarities(gendered_text,target_text):\n",
    "    result = {}\n",
    "    result['Target_Texts'] = target_text\n",
    "    cos = nn.CosineSimilarity(dim=1)\n",
    "    for gt in gendered_text:\n",
    "        encoded_input = tokenizer(gt, return_tensors='pt')\n",
    "        output = model(**encoded_input)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        gt_embedding = last_hidden_state.mean(axis=1)\n",
    "        result[gt] = []\n",
    "        for tt in target_text:\n",
    "            encoded_input = tokenizer(tt, return_tensors='pt')\n",
    "            output = model(**encoded_input)\n",
    "            last_hidden_state = output.last_hidden_state\n",
    "            tt_embedding = last_hidden_state.mean(axis=1)\n",
    "            sim = cos(gt_embedding, tt_embedding)\n",
    "            result[gt].append(sim.item())\n",
    "    \n",
    "    temp = {}\n",
    "    temp['Gendered_Texts'] = pd.DataFrame(result).set_index('Target_Texts')\n",
    "    result = pd.concat(temp, axis=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0488e5f-cec5-4bb4-979c-af21b64929c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b0554f5-8e00-4efd-9bb7-d26fbace36ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# DistilBert Parameters: 66M (Remember from the lecture that BERT has around 110M parameters)\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModel.from_pretrained(MODEL_TYPE)\n",
    "print(f\"# DistilBert Parameters: {round(model.num_parameters() / 1_000_000)}M (Remember from the lecture that BERT has around 110M parameters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a8a8260-1a38-4fc1-88dc-e5cd5efac10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"He is walking.\" \n",
    "text2 = \"She is walking.\"\n",
    "text3 = \"The dancer is walking.\" \n",
    "text4 = \"The chef is walking.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be2b1215-efc1-4e40-8fb2-3e938b4ae259",
   "metadata": {},
   "outputs": [],
   "source": [
    "gendered_text = [text1,text2]\n",
    "target_text = [text3,text4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3aa0f8c9-51cd-4e9d-9eb7-7587c59f76fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Gendered_Texts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>He is walking.</th>\n",
       "      <th>She is walking.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target_Texts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>The dancer is walking.</th>\n",
       "      <td>0.889623</td>\n",
       "      <td>0.907225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The chef is walking.</th>\n",
       "      <td>0.879000</td>\n",
       "      <td>0.875073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Gendered_Texts                \n",
       "                       He is walking. She is walking.\n",
       "Target_Texts                                         \n",
       "The dancer is walking.       0.889623        0.907225\n",
       "The chef is walking.         0.879000        0.875073"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_similarities(gendered_text,target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d81cdf-7246-4243-996f-3e9ed6efed5c",
   "metadata": {},
   "source": [
    "## Using Sentence-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50a34f59-2a2b-4709-b15f-cebfffd49648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4860d71-5c49-4e22-9549-733e3324ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_similarities_sentBERT(gendered_text,target_text,tokenizer,model):\n",
    "    result = {}\n",
    "    result['Target_Texts'] = target_text\n",
    "    cos = nn.CosineSimilarity(dim=1)\n",
    "    for gt in gendered_text:\n",
    "        encoded_input = tokenizer(gt, padding=True, truncation=True, return_tensors='pt')\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "        # Perform pooling. In this case, max pooling.\n",
    "        gt_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        # gt_embedding = last_hidden_state.mean(axis=1)\n",
    "        result[gt] = []\n",
    "        for tt in target_text:\n",
    "            encoded_input = tokenizer(tt, padding=True, truncation=True, return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                model_output = model(**encoded_input)\n",
    "            tt_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "            sim = cos(gt_embedding, tt_embedding)\n",
    "            result[gt].append(sim.item())\n",
    "    \n",
    "    temp = {}\n",
    "    temp['Gendered_Texts'] = pd.DataFrame(result).set_index('Target_Texts')\n",
    "    result = pd.concat(temp, axis=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14a18d94-b1b5-45a3-958c-f2ae5a4326af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c366b833-04fc-466d-b3cd-507b350036ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Gendered_Texts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>He is walking.</th>\n",
       "      <th>She is walking.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target_Texts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>The dancer is walking.</th>\n",
       "      <td>0.818181</td>\n",
       "      <td>0.845284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The chef is walking.</th>\n",
       "      <td>0.741682</td>\n",
       "      <td>0.655532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Gendered_Texts                \n",
       "                       He is walking. She is walking.\n",
       "Target_Texts                                         \n",
       "The dancer is walking.       0.818181        0.845284\n",
       "The chef is walking.         0.741682        0.655532"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_similarities_sentBERT(gendered_text,target_text,tokenizer,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e80e907-ce84-4cff-a45f-e4d6379ab100",
   "metadata": {},
   "source": [
    "## Fine-Tuning using pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "608de1b0-d385-423d-80ba-89a96bff8a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f66ee4-8e6e-41d5-915b-2c625360ad89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "709bc0c3-c568-4a83-a6f0-098f02b554ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': \"Owning a driving range inside the city limits is like a license to print money.  I don't think I ask much out of a driving range.  Decent mats, clean balls and accessible hours.  Hell you need even less people now with the advent of the machine that doles out the balls.  This place has none of them.  It is april and there are no grass tees yet.  BTW they opened for the season this week although it has been golfing weather for a month.  The mats look like the carpet at my 107 year old aunt Irene's house.  Worn and thread bare.  Let's talk about the hours.  This place is equipped with lights yet they only sell buckets of balls until 730.  It is still light out.  Finally lets you have the pit to hit into.  When I arrived I wasn't sure if this was a driving range or an excavation site for a mastodon or a strip mining operation.  There is no grass on the range. Just mud.  Makes it a good tool to figure out how far you actually are hitting the ball.  Oh, they are cash only also.\\\\n\\\\nBottom line, this place sucks.  The best hope is that the owner sells it to someone that actually wants to make money and service golfers in Pittsburgh.\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed898e51-0c7c-4e45-b062-1e44eeae6204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114be062-aa2c-430e-b7d2-283dbf5a9026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f00545c-cf91-4cd8-963c-6c3c9b970738",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e894cb9-0c96-42fd-b95a-87ce90968665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16c0d95d-025d-4de3-8809-2f79fb79dce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5bcc657b218478aa3ce6c999afe971a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8925bdbc-e965-4405-83c7-95742ab92522",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acf92b59-d78b-4c42-b7a8-98524c1366b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a93fe28-3f43-4cf4-907a-ce7dcf3ef752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34d1dab7-06c3-4be3-b125-8666590bbe55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662dc9041f424f5c9683d9d03d79caa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df850623-ceca-4647-a6cf-f350d6811afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdf9aa5c-e4a7-44fd-b8a5-0221e4f24de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b96cae95-0044-4f66-8883-ddd2c583e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3908b6c8-ef1a-4a62-ba71-81f9aa0a3f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 28:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39, training_loss=1.5139410556891026, metrics={'train_runtime': 1740.6311, 'train_samples_per_second': 0.172, 'train_steps_per_second': 0.022, 'total_flos': 78935442739200.0, 'train_loss': 1.5139410556891026, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbc9841-8c87-41d5-8008-97ad168dc78c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
